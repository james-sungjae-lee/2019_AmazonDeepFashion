{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Intelligence Assignment 3\n",
    "## No1. Neural Net\n",
    "## 20132651 Sungjae Lee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN 을 구현하기 위해 pytorch 의 neural net 패키지를 불러옵니다.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolution 층 2개와 Fully Connected 층 3개로 이루어진 Neural Net 을 하나의 클래스로 정의합니다.\n",
    "## 해당 클래스에는 전방 전파를 수행하기 위한 forward 함수가 정의되며\n",
    "## 컨볼루션 연산, 완전연결 다층 퍼셉트론 연산, ReLU 함수의 적용, Pooling 연산등의 수행 과정이 나타납니다.\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        ## pytorch 의 nn.Conv2d 는 2차원 Convolution 연산을 하는 함수입니다.\n",
    "        ## 3개의 변수가 반드시 들어가며, 앞에서부터 입력 개수, 출력 개수, 커널 크기입니다.\n",
    "        ## 그 외에 stride, padding, bias 등에 대해 조정할 수 있습니다.\n",
    "        \n",
    "        ## conv1 은 1개의 이미지 입력을 5 x 5 커널(필터)에 통과시켜 6개의 출력을 만드는 컨볼루션 층입니다.\n",
    "        ## conv2 는 위에서의 6개 출력을 입력으로 받아, 5 x 5 커널에 통과시켜 16개의 출력을 만드는 컨볼루션 층입니다.\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        ## pytorch 의 nn.Linear 는 완전연결 다층 퍼셉트론 연산을 하는 함수입니다.\n",
    "        ## 필수적으로 in_features 와 out_features 매개변수가 입력되어\n",
    "        ## N 개의 입력에 대해 M 개의 출력으로 Fully Connected 된 층을 형성합니다.\n",
    "        \n",
    "        ## fc1 ~ fc3 는 각각 400 x 120, 120 x 84, 84 x 10 으로 이루어진 층입니다.\n",
    "        ## 마지막의 10개 출력은 하나의 이미지가 10개 레이블중 어디에 가까운지 출력하기 위함입니다.\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ## 전방 전파를 수행하기 위한 함수 forward 를 정의합니다.\n",
    "        ## 해당 CNN에서 Pooling 작업은 nn.functional.max_pool2d 함수를 이용하여 진행합니다.\n",
    "        ## 2 x 2 커널에서 최대값을 뽑아내는 Max Pooling 작업입니다.\n",
    "        \n",
    "        ## 먼저 입력 x 에 conv 함수와 relu 함수를 적용시킨 다음, Pooling 을 시행합니다.\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        \n",
    "        ## Pooling 층의 작동에 있어서, 2 x 2 커널을 만드는 것은 단순히 2 값을 주는 것으로도 가능합니다.\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        \n",
    "        ## view 함수는 pytorch 에서 Tensor의 형태를 변환시키는 함수 입니다.\n",
    "        ## numpy 의 reshape 와 동일한 기능이라 볼 수 있습니다.\n",
    "        ## 이 때, -1 값을 주면 뒤의 값에 따라 자동적으로 크기를 계산하여 reshape 하게 됩니다.\n",
    "        \n",
    "        ## conv, relu, pool 연산을 마친 x 를 한 줄의 출력으로 변환합니다.\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        ## relu 를 사용한 완전연결 다층 퍼셉트론으로 최종 출력까지 진행합니다.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        # batch 를 제외한 나머지 feature 의 개수를 곱연산한 값을 반환합니다.\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "## net 이름으로 Net 클래스를 하나 생성합니다.    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 화면 출력 확인 및 의미를 서술"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## 위에서 정의한 CNN 의 전체 구조를 한 눈에 확인할 수 있습니다.\n",
    "## conv1 과 conv2 의 컨볼루션층을 통과하며 학습을 진행한 다음,\n",
    "## fc 층들을 통과하며 최종적으로 10개의 클래스로 분류되도록 만들어져 있습니다.\n",
    "## 이 때, 여기서는 보이지 않지만 ReLU 함수를 이용하여 Gradient Vanishing 문제를 해결하고\n",
    "## Max Pooling 을 이용해 feature 를 요약하는 과정도 포함됩니다.\n",
    "## 위에서는 정의하지 않았지만 stride(보폭)과 bias 가 기본값으로 설정된 것을 확인할 수 있습니다.\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 정의된 컨볼루션 신경망의 구조 설명 ( 위의 AlexNet 그림 참고 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.parameters() 를 사용하여 정의된 신경망의 학습가능한 매개변수들을 확인 가능합니다.\n",
    "\n",
    "params = list(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 화면 출력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# 총 10개의 학습층이 존재하며, 각각의 학습 weight 크기를 확인할 수 있습니다.\n",
    "# 그중에서도 첫 번째 컨볼루션 층에서는 5 x 5 크기의 커널이 6개 존재합니다.\n",
    "\n",
    "print(len(params))\n",
    "for i in range(len(params)):\n",
    "    print(params[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 32*32 입력이 주어졌을때를 가정하고 CNN이 정상적으로 작동하는지 확인합니다.\n",
    "# 참고로 크기가 다른 입력을 받을 때는 입력의 크기를 재조정하거나 신경망을 수정해야 합니다.\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 화면 출력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0629, -0.0513, -0.0383, -0.0380, -0.0024,  0.1584, -0.0402, -0.0381,\n",
      "         -0.1104,  0.0336]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "## 정상적으로 10개의 레이블에 대해 어떤 레이블에 가까운지에 대한 수치가\n",
    "## 출력으로 나오는 것을 확인할 수 있습니다.\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오류 역전파를 통해 그레디언트를 구하기 전에, 모든 가중치의 그레디언트 버퍼를 초기화합니다.\n",
    "# 이 때, zero_grad 함수를 이용하면 가중치를 손쉽게 초기화할 수 있습니다.\n",
    "# backward 를 이용하여 역전파를 진행합니다.\n",
    "\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수를 정의하고, 임의의 값들에 대해서 오차 결과를 확인합니다.\n",
    "# nn 패키지는 많이 사용되는 손실함수들을 제공하며, 여기서는 단순하게 MSE Loss Function 을 사용합니다.\n",
    "\n",
    "output = net(input)\n",
    "target = torch.randn(10) # 예시를 위해 랜덤한 값을 target 으로 입력합니다.\n",
    "target = target.view(1, -1) # output 과 동일한 크기로 변형합니다.\n",
    "criterion = nn.MSELoss() # criterion 은 기준이란 의미로, 목적 함수를 의미하는 듯 합니다.\n",
    "\n",
    "loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 화면 출력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1344, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "## 위에서 정의한 목적 함수, MSE 를 이용하여 loss 를 계산한 결과를 출력합니다.\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 앞에 코드에서 언급한 것과 같이 오류 역전파하기 전, 그레디언트를 초기화해야 합니다.\n",
    "## backward() 수행 후 어떤 변화가 있는지 확인하고, 초기화의 필요성을 확인해 봅니다.\n",
    "\n",
    "## zero_grad() 를 이용하여 모든 parameter 의 가중치를 0으로 초기화 합니다. \n",
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 화면 출력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "## 정상적으로 가중치가 0으로 초기화 되었는지 확인하기 위해 출력해 봅니다.\n",
    "## 이 때, .grad 를 이용하면 확인이 가능합니다.\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 가중치를 초기화 한 다음, backward 를 이용하여 오류 역전파를 시행합니다.\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 화면 출력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0071, -0.0040,  0.0010, -0.0100, -0.0043,  0.0089])\n"
     ]
    }
   ],
   "source": [
    "## backward 를 진행한 다음, 가중치를 출력하여 확인합니다.\n",
    "## 0으로 초기화 되었던 가중치들이 오류 역전파를 통해 새로운 값을 가지게 되었습니다.\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스토캐스틱 경사하강볍 : ((미래)가중치 = (현재)가중치 - 학습률 * 그레디언트)\n",
    "# 해당 공식을 이용하여 가중치를 갱신하는 코드는 다음과 같습니다.\n",
    "\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "    ## sub_ 메소드를 이용하여 코드를 효율적으로 구현한 점이 눈에 띕니다.\n",
    "    ## 찾아본 결과 sub_ 은 sub 의 inplace 버전이라고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하지만 실제로는 torch.optim 에서 구현되어 있는 SDG, Adam, RMSProp 등을 사용합니다.\n",
    "# 아래는 오류 역전파에서 최적화하는 방법을 보인 예제 코드입니다.\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "## optim 의 SGD 를 사용하여 스토캐스틱 경사 하강을 optimizer 로 정의합니다.\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)\n",
    "\n",
    "## 아래는 실제 학습이 진행되는 코드입니다.\n",
    "optimizer.zero_grad() \n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "## 가중치 초기화, 목적함수 정의, 오류 역전파 등의 과정은 동일합니다\n",
    "## optimizer.step() 을 이용하여 갱신하는 점이 다른 것을 확인할 수 있습니다.\n",
    "optimizer.step() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
